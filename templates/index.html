<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>Oral Exam Tool ‚Äì Iteration 1</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <style>
    body { font-family: system-ui, sans-serif; max-width: 680px; margin: 2rem auto; padding: 0 1rem; }
    button, select { font-size: 1rem; padding: .6rem .9rem; margin-right: .5rem; }
    .row { margin: 1rem 0; }
    .box { border: 1px solid #ddd; border-radius: 8px; padding: .75rem; background:#fff; min-height: 2.2rem; }
    .muted { color:#666; font-size:.95rem; }
    .ok { color:#0a0; }
    .err { color:#b00; }
  </style> <!-- This sets the page's layout and inline CSS -->
</head>
<body>
  <h1>Oral Exam Tool ‚Äì Iteration 1</h1> <!-- This is the Title of the page, and Sub-heading -->
  <p class="muted">Flow: Start ‚Üí Stop ‚Üí Transcribe (STT) ‚Üí Get Feedback ‚Üí Speak (TTS).</p>

  <div class="row">
    <button id="startBtn">üéôÔ∏è Start Recording</button>
    <button id="stopBtn" disabled>‚èπÔ∏è Stop & Process</button>
    <select id="lang">
      <option value="en-GB" selected>English (UK)</option>
      <!-- Buttons starting and stopping the recording also language selection -->
    </select>
  </div>

  <div class="row muted" id="status">Idle.</div> <!-- A small text area that shows the current status message (recording, transcribing etc) -->

  <div class="row">
    <strong>Transcript</strong>
    <div id="transcript" class="box"></div>
  </div>

  <div class="row">
    <strong>Feedback</strong>
    <div id="feedback" class="box"></div>
  </div>
<!-- Empty boxes that will later display the recognised speech and the feedback text -->
  <div class="row">
    <audio id="player" controls style="width:100%;"></audio>
  </div>
<!-- An audio player that will play the synthesised feedback audio -->
  <script>
    // This code is from ChatGPT
    // Retrieves all buttons and containers by ID and prepares variables for recording
    const startBtn = document.getElementById('startBtn');
    const stopBtn  = document.getElementById('stopBtn');
    const statusEl = document.getElementById('status');
    const transcriptEl = document.getElementById('transcript');
    const feedbackEl   = document.getElementById('feedback');
    const player       = document.getElementById('player');
    const langSel      = document.getElementById('lang');

    // ---------- State ----------
    let mediaStream = null;
    let recorder = null;
    let chunks = [];
    // Media stream is what the browser gives you after the mic permission,
    // recorder is the engine that encodes audio and emits chunks,
    // Chunks collects those pieces until you stop recording

    // ---------- Helpers ----------
    function pickMime() {
      const candidates = [
        'audio/webm;codecs=opus', // Chrome/Edge
        'audio/webm',
        'audio/mp4',              // Safari fallback
        'audio/ogg;codecs=opus'   // Firefox
      ];
      for (const c of candidates) {
        if (MediaRecorder.isTypeSupported && MediaRecorder.isTypeSupported(c)) return c;
      }
      return '';
    }
    // Checks which MIME type the current browser can encode with MediaRecorder.
    // Tries best - acceptable fallbacks
    // returned string feeds into (new MediaRecorder(stream, {mimetype}))

    // This Code is from ChatGPT
    function setStatus(msg, cls = '') {
      statusEl.className = 'muted ' + cls;
      statusEl.textContent = msg;
    }
    // Central place to update the status line colour.
    // This code is from ChatGpt
    function stopAudio() {
      try {
        player.pause(); player.currentTime = 0;
        const src = player.src; player.src = ''; player.src = src;
      } catch (_) {}
    }
    // Stops any currently playing feedback and resets the audio element so a new blob will load cleanly.

    // ---------- Recording ----------
    // This code is from ChatGPT
    async function startRecording() {
      setStatus('Requesting microphone‚Ä¶');
      stopAudio();
    // Requests the mic (prompts user) if allowed creates a MediaRecorder.
      if (!navigator.mediaDevices?.getUserMedia) {
        setStatus('Microphone not available in this browser.', 'err');
        return;
      }
    // Each time the encoder has data, it pushes a chunk into chunks.
      try {
        mediaStream = await navigator.mediaDevices.getUserMedia({ audio: true });
      } catch (e) {
        setStatus('Microphone permission denied.', 'err');
        return;
      }

      const mimeType = pickMime();
      try {
        recorder = new MediaRecorder(mediaStream, mimeType ? { mimeType } : undefined);
      } catch (e) {
        setStatus('MediaRecorder not supported for this mime.', 'err');
        return;
      }

      chunks = [];
      recorder.ondataavailable = (e) => { if (e.data && e.data.size > 0) chunks.push(e.data); };

      recorder.start();
      startBtn.disabled = true;
      stopBtn.disabled  = false;
      setStatus(`Recording‚Ä¶ ${mimeType || '(default mime)'}`, 'ok');
    }
    // Disables/Enables buttons to prevent double-click race conditions.

    // THis code is from ChatGPT
    function stopRecording() {
      return new Promise((resolve) => {
        if (!recorder) { resolve(null); return; }
        recorder.onstop = () => {
          try { mediaStream.getTracks().forEach(t => t.stop()); } catch (_) {}
          const blob = new Blob(chunks, { type: (recorder && recorder.mimeType) ? recorder.mimeType : 'audio/webm' });
          console.log('Recorder mime:', recorder ? recorder.mimeType : '(none)', 'size:', blob.size);
          resolve(blob);
        };
        try { recorder.stop(); } catch (_) { resolve(null); }
      });
    }
    // Stops the recorder and mic tracks, then joins all chunks into a single Blob
    // The Blobs type is set to the recorder's MIME when available (helps the server know what it is)

    // ---------- Pipeline: STT -> Feedback -> TTS ----------
    // This code is from ChatGPT
    // Quick sanity check, tiny blobs are often silence or failed captures.
    async function processAudioBlob(blob) {
      if (!blob || blob.size < 2000) {
        setStatus(`No usable audio captured (size=${blob ? blob.size : 0}).`, 'err');
        transcriptEl.textContent = '(no speech detected)';
        return;
      }

      // 1) STT
      // This code is from ChatGpt
      setStatus('Transcribing‚Ä¶');
      const filename = ((recorder && recorder.mimeType) || '').includes('mp4') ? 'speech.m4a' : 'speech.webm';
      const form = new FormData();
      form.append('file', blob, filename);
      form.append('lang', langSel.value || 'en-GB');
    // Constructs a FromData multipart upload with file and lang
    // Picks a filename extension that matches the container (.m4a if the MIME hinted mp4 otherwise .webm).
      let transcript = '';
      try {
        const sttResp = await fetch('/api/stt', { method: 'POST', body: form });
        const sttJson = await sttResp.json();
        console.log('STT:', sttJson);
        if (!sttResp.ok) throw new Error(sttJson.error || 'STT failed');
        transcript = (sttJson.transcript || '').trim();
      } catch (e) {
        setStatus('Transcription failed: ' + e.message, 'err');
        return;
      }
      transcriptEl.textContent = transcript || '(empty transcript)';
      if (!transcript) { setStatus('Empty transcript.', 'err'); return; }
    // Posts to your Flask /api/stt endpoint
    // expects JSON like {"transcript" : "...", "lang_used": "en"}.
    // Shows transcript or aborts if empty.

      // 2) Feedback
      // This code is from ChatGPT
      setStatus('Getting feedback‚Ä¶');
      let feedback = '';
      try {
        const fbResp = await fetch('/api/feedback', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({ transcript, prompt: '' })
        });
        const fbJson = await fbResp.json();
        console.log('Feedback:', fbJson);
        if (!fbResp.ok) throw new Error(fbJson.error || 'Feedback failed');
        feedback = (fbJson.feedback || '').trim();
      } catch (e) {
        setStatus('Feedback failed: ' + e.message, 'err');
        return;
      }
      feedbackEl.textContent = feedback || '(no feedback returned)';

      // Sends tanscript to /api/feedback.
        // Expects JSON like {"feedback": "..."}.
        // Renders the feedback in the UI.

      // 3) TTS
      // This code is from ChatGPT
      setStatus('Speaking feedback‚Ä¶');
      try {
        const ttsResp = await fetch('/api/tts', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({ text: feedback, voice: 'alloy' }) // server returns MP3
});


        if (!ttsResp.ok) {
          const j = await ttsResp.json().catch(()=>({}));
          throw new Error(j.details || j.error || 'TTS failed');
        }
        const audioBlob = await ttsResp.blob();
        player.src = URL.createObjectURL(audioBlob);
        await player.play().catch(()=>{ /* gesture may be needed first time */ });
      } catch (e) {
        setStatus('TTS failed: ' + e.message, 'err');
        return;
      }

      setStatus('Done.', 'ok');
    }

    // Posts feedback text to /api/tts, which returns audio bytes (MP3)
        // Converts the response to a Blob, makes an object URL, and sets audio.src.
        // Calls play() (may silently fail on first use without a user gesture; already have a click, so it's usually fine).


    // ---------- Wire up buttons ----------
    // This code is from ChatGPT
    startBtn.addEventListener('click', startRecording);
    stopBtn.addEventListener('click', async () => {
      stopBtn.disabled = true;
      const blob = await stopRecording();
      startBtn.disabled = false;
      await processAudioBlob(blob);
    });
    // Connects UI actions to the recording/pipline functions
    // Handles a simple enabled/disabled state to keep UX stable
  </script>
</body>
</html>
